{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ADNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HnVFCZQx3ke",
        "outputId": "f84b87c9-eed0-47e7-de84-60a0136b34ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"./gdrive/MyDrive/iitisoc/\""
      ],
      "metadata": {
        "id": "F677EG9vx7xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, clean_dir, noisy_dir):\n",
        "        super(ImageDataset, self).__init__()\n",
        "        self.clean_file_filenames = [os.path.join(clean_dir, x) for x in os.listdir(clean_dir)]\n",
        "        self.noisy_file_filenames = [os.path.join(noisy_dir, x) for x in os.listdir(noisy_dir)]\n",
        "        self.transform = transforms.ToTensor()\n",
        "    def __getitem__(self, index):\n",
        "        clean = Image.open(self.clean_file_filenames[index])\n",
        "        noisy = Image.open(self.noisy_file_filenames[index])\n",
        "        return self.transform(noisy), self.transform(clean)\n",
        "    def __len__(self):\n",
        "        return len(self.clean_file_filenames)"
      ],
      "metadata": {
        "id": "jKbzoKNYx9fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"ADNet Model.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1TwMBYw0Dos-VmalGcf2GAThpdtOzzYx-\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv_BN_Relu_first(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,padding,groups,bias):\n",
        "        super(Conv_BN_Relu_first,self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = 64\n",
        "        groups =1 \n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=features, kernel_size=kernel_size, padding=padding,groups=groups, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self,x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "class Conv_BN_Relu_other(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,padding,groups,bias):\n",
        "        super(Conv_BN_Relu_other,self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = out_channels\n",
        "        groups =1 \n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=features, kernel_size=kernel_size, padding=padding,groups=groups, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(features)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self,x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,padding,groups,bais):\n",
        "        super(Conv,self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = 1\n",
        "        groups =1 \n",
        "        self.conv = nn.Conv2d(in_channels=3, out_channels=features, kernel_size=kernel_size, padding=padding,groups=groups, bias=False)\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class Self_Attn(nn.Module):\n",
        "    def __init__(self,in_dim):\n",
        "        super(Self_Attn,self).__init__()\n",
        "        self.chanel_in = in_dim\n",
        "        self.query_conv = nn.Conv2d(in_channels=in_dim,out_channels=in_dim//8,kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels=in_dim,out_channels=in_dim//8,kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels=in_dim,out_channels=in_dim,kernel_size=1)\n",
        "        self.gamma=nn.Parameter(torch.zeros(1))\n",
        "        self.softmax=nn.Softmax(dim=-1)\n",
        "    def forward(self,x):\n",
        "        m_batchsize, C, width,height = x.size()\n",
        "        proj_query = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1)\n",
        "        proj_key = self.key_conv(x).view(m_batchsize,-1,width*height)\n",
        "        \n",
        "        energy = torch.bmm(proj_query,proj_key)\n",
        "        attention = self.softmax(energy)\n",
        "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) \n",
        "        out = torch.bmm(proj_value,attention.permute(0,2,1))\n",
        "        out = out.view(m_batchsize,C,width,height)\n",
        "        out = self.gamma*out + x\n",
        "        return out, attention\n",
        "\n",
        "class ADNet(nn.Module):\n",
        "  def __init__(self, channels, num_of_layers=15):\n",
        "        super(ADNet, self).__init__()\n",
        "        kernel_size = 3\n",
        "        padding = 1\n",
        "        features = 64\n",
        "        groups =1 \n",
        "        layers = []\n",
        "        kernel_size1 = 1\n",
        "        self.conv1_1 = nn.Sequential(nn.Conv2d(in_channels=channels,out_channels=features,kernel_size=kernel_size,padding=padding,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_2 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=2,groups=groups,bias=False,dilation=2),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_3 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_4 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_5 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=2,groups=groups,bias=False,dilation=2),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_4 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_5 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=2,groups=groups,bias=False,dilation=2),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_6 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_7 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=padding,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_8 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_9 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=2,groups=groups,bias=False,dilation=2),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_10 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_11 = nn.Sequential(nn.Conv2d(in_channels=features,out_channels=features,kernel_size=kernel_size,padding=1,groups=groups,bias=False),nn.BatchNorm2d(features),nn.ReLU(inplace=True))\n",
        "        self.conv1_12 = nn.Conv2d(in_channels=features,out_channels=3,kernel_size=kernel_size,padding=1,groups=groups,bias=False)\n",
        "        self.conv3 = nn.Conv2d(in_channels=6,out_channels=3,kernel_size=1,stride=1,padding=0,groups=1,bias=True)\n",
        "        self.ReLU = nn.ReLU(inplace=True)\n",
        "        self.Tanh= nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                \n",
        "                m.weight.data.normal_(0, (2 / (9.0 * 64)) ** 0.5)\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.normal_(0, (2 / (9.0 * 64)) ** 0.5)\n",
        "                clip_b = 0.025\n",
        "                w = m.weight.data.shape[0]\n",
        "                for j in range(w):\n",
        "                    if m.weight.data[j] >= 0 and m.weight.data[j] < clip_b:\n",
        "                        m.weight.data[j] = clip_b\n",
        "                    elif m.weight.data[j] > -clip_b and m.weight.data[j] < 0:\n",
        "                        m.weight.data[j] = -clip_b\n",
        "                m.running_var.fill_(0.01)\n",
        "\n",
        "  def _make_layers(self, block,features, kernel_size, num_of_layers, padding=1, groups=1, bias=False):\n",
        "      layers = []\n",
        "      for _ in range(num_of_layers):\n",
        "        layers.append(block(in_channels=features, out_channels=features, kernel_size=kernel_size, padding=padding, groups=groups, bias=bias))\n",
        "      return nn.Sequential(*layers)                \n",
        "    \n",
        "      \n",
        "\t  \n",
        "  def forward(self, x):\n",
        "        input = x \n",
        "        x1 = self.conv1_1(x)\n",
        "        x1 = self.conv1_2(x1)\n",
        "        x1 = self.conv1_3(x1)\n",
        "        x1 = self.conv1_4(x1)\n",
        "        x1 = self.conv1_5(x1)\n",
        "        x1 = self.conv1_6(x1)\n",
        "        x1 = self.conv1_7(x1)   \n",
        "        x1 = self.conv1_8(x1)\n",
        "        x1 = self.conv1_9(x1)\n",
        "        x1 = self.conv1_10(x1)\n",
        "        x1 = self.conv1_11(x1)\n",
        "        x1 = self.conv1_12(x1)\n",
        "        \n",
        "        out = torch.cat([x,x1],1)\n",
        "        out= self.Tanh(out)\n",
        "        out = self.conv3(out)\n",
        "        out = out*x1\n",
        "        out2 = x - out\n",
        "        return out2"
      ],
      "metadata": {
        "id": "_0NVP6nvyBww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "\n",
        "from math import log10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_dataset = ImageDataset(path + \"/large/clean/\", path + \"/large/noisy/\")\n",
        "test_dataset = ImageDataset(path + \"/large/test_clean/\", path + \"/large/test_noisy/\")\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "training_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
        "testing_data_loader = DataLoader(dataset=test_dataset, batch_size=batch_size)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ADNet(channels=3, num_of_layers=4).to(device)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "lr = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    epoch_loss = 0\n",
        "    for i, data in enumerate(training_data_loader, 1):\n",
        "        input = data[0].cuda()\n",
        "        target = data[1].cuda()\n",
        "        out = model(input)\n",
        "\n",
        "        loss = criterion(out, target)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, i, len(training_data_loader), loss.item()))\n",
        "    \n",
        "    print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))\n",
        "\n",
        "def validate():\n",
        "    avg_psnr = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testing_data_loader:\n",
        "            input = data[0].cuda()\n",
        "            output = model(input)\n",
        "            mse = criterion(output, input)\n",
        "            psnr = 10 * log10(1 / mse.item())\n",
        "            avg_psnr += psnr\n",
        "    print(\"===> Avg. PSNR: {:.4f} dB\".format(avg_psnr / len(testing_data_loader)))\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def save(state, epoch):\n",
        "    model_out_path = \"./model_epoch_{}.pth\".format(epoch)\n",
        "    torch.save(state, model_out_path)\n",
        "    print(\"Checkpoint saved to {}\".format(model_out_path))\n",
        "\n",
        "def training():\n",
        "    checkpoint = torch.load(\"./model_epoch_60.pth\")\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    epoch = checkpoint['epoch']\n",
        "\n",
        "    num_epochs = 20\n",
        "    for epoch in range(61, num_epochs + 61):\n",
        "        train(epoch)\n",
        "        if epoch % 10 == 0:\n",
        "            validate()\n",
        "            save({\n",
        "                'epoch': epoch + 1,\n",
        "                'arch': model,\n",
        "                'state_dict': model.state_dict(),\n",
        "                'optimizer' : optimizer.state_dict(),\n",
        "            }, epoch)\n",
        "\n",
        "def testing():\n",
        "    for image_number in range(320,330):\n",
        "        trans = transforms.ToPILImage()\n",
        "        model = torch.load(\"./model_epoch_20.pth\")\n",
        "        model = model['arch']\n",
        "        loader = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "        image = Image.open(f\"../new_data/final_test/noisy_{image_number}.jpg\")\n",
        "        image = loader(image).float()\n",
        "        print(image.shape)\n",
        "        image = image.unsqueeze(0)\n",
        "        out = torch.clamp(image-model(image), 0., 1.)\n",
        "        out = out.squeeze(0)\n",
        "        print(out.shape)\n",
        "        trans(out).save(f'../new_data/final_test/clean_{image_number}.jpg')"
      ],
      "metadata": {
        "id": "TF8MruBVyNOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6PdbPpByX8l",
        "outputId": "3ff0f7df-1d4f-4d6e-bfeb-3e6b678e4fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Epoch[61](1/32): Loss: 0.0204\n",
            "===> Epoch[61](2/32): Loss: 0.0245\n",
            "===> Epoch[61](3/32): Loss: 0.0189\n",
            "===> Epoch[61](4/32): Loss: 0.0390\n",
            "===> Epoch[61](5/32): Loss: 0.0232\n",
            "===> Epoch[61](6/32): Loss: 0.0359\n",
            "===> Epoch[61](7/32): Loss: 0.0339\n",
            "===> Epoch[61](8/32): Loss: 0.0404\n",
            "===> Epoch[61](9/32): Loss: 0.0393\n",
            "===> Epoch[61](10/32): Loss: 0.0771\n",
            "===> Epoch[61](11/32): Loss: 0.0439\n",
            "===> Epoch[61](12/32): Loss: 0.0404\n",
            "===> Epoch[61](13/32): Loss: 0.0370\n",
            "===> Epoch[61](14/32): Loss: 0.0493\n",
            "===> Epoch[61](15/32): Loss: 0.0165\n",
            "===> Epoch[61](16/32): Loss: 0.0156\n",
            "===> Epoch[61](17/32): Loss: 0.0150\n",
            "===> Epoch[61](18/32): Loss: 0.0253\n",
            "===> Epoch[61](19/32): Loss: 0.0238\n",
            "===> Epoch[61](20/32): Loss: 0.0557\n",
            "===> Epoch[61](21/32): Loss: 0.0434\n",
            "===> Epoch[61](22/32): Loss: 0.0466\n",
            "===> Epoch[61](23/32): Loss: 0.0435\n",
            "===> Epoch[61](24/32): Loss: 0.0311\n",
            "===> Epoch[61](25/32): Loss: 0.0473\n",
            "===> Epoch[61](26/32): Loss: 0.0370\n",
            "===> Epoch[61](27/32): Loss: 0.0334\n",
            "===> Epoch[61](28/32): Loss: 0.0352\n",
            "===> Epoch[61](29/32): Loss: 0.0283\n",
            "===> Epoch[61](30/32): Loss: 0.0344\n",
            "===> Epoch[61](31/32): Loss: 0.0415\n",
            "===> Epoch[61](32/32): Loss: 0.0329\n",
            "===> Epoch 61 Complete: Avg. Loss: 0.0353\n",
            "===> Epoch[62](1/32): Loss: 0.0203\n",
            "===> Epoch[62](2/32): Loss: 0.0245\n",
            "===> Epoch[62](3/32): Loss: 0.0187\n",
            "===> Epoch[62](4/32): Loss: 0.0389\n",
            "===> Epoch[62](5/32): Loss: 0.0231\n",
            "===> Epoch[62](6/32): Loss: 0.0359\n",
            "===> Epoch[62](7/32): Loss: 0.0339\n",
            "===> Epoch[62](8/32): Loss: 0.0404\n",
            "===> Epoch[62](9/32): Loss: 0.0393\n",
            "===> Epoch[62](10/32): Loss: 0.0770\n",
            "===> Epoch[62](11/32): Loss: 0.0439\n",
            "===> Epoch[62](12/32): Loss: 0.0404\n",
            "===> Epoch[62](13/32): Loss: 0.0370\n",
            "===> Epoch[62](14/32): Loss: 0.0492\n",
            "===> Epoch[62](15/32): Loss: 0.0165\n",
            "===> Epoch[62](16/32): Loss: 0.0156\n",
            "===> Epoch[62](17/32): Loss: 0.0150\n",
            "===> Epoch[62](18/32): Loss: 0.0253\n",
            "===> Epoch[62](19/32): Loss: 0.0238\n",
            "===> Epoch[62](20/32): Loss: 0.0556\n",
            "===> Epoch[62](21/32): Loss: 0.0434\n",
            "===> Epoch[62](22/32): Loss: 0.0465\n",
            "===> Epoch[62](23/32): Loss: 0.0434\n",
            "===> Epoch[62](24/32): Loss: 0.0311\n",
            "===> Epoch[62](25/32): Loss: 0.0472\n",
            "===> Epoch[62](26/32): Loss: 0.0369\n",
            "===> Epoch[62](27/32): Loss: 0.0333\n",
            "===> Epoch[62](28/32): Loss: 0.0352\n",
            "===> Epoch[62](29/32): Loss: 0.0282\n",
            "===> Epoch[62](30/32): Loss: 0.0344\n",
            "===> Epoch[62](31/32): Loss: 0.0414\n",
            "===> Epoch[62](32/32): Loss: 0.0330\n",
            "===> Epoch 62 Complete: Avg. Loss: 0.0353\n",
            "===> Epoch[63](1/32): Loss: 0.0203\n",
            "===> Epoch[63](2/32): Loss: 0.0244\n",
            "===> Epoch[63](3/32): Loss: 0.0187\n",
            "===> Epoch[63](4/32): Loss: 0.0389\n",
            "===> Epoch[63](5/32): Loss: 0.0231\n",
            "===> Epoch[63](6/32): Loss: 0.0359\n",
            "===> Epoch[63](7/32): Loss: 0.0339\n",
            "===> Epoch[63](8/32): Loss: 0.0403\n",
            "===> Epoch[63](9/32): Loss: 0.0392\n",
            "===> Epoch[63](10/32): Loss: 0.0769\n",
            "===> Epoch[63](11/32): Loss: 0.0438\n",
            "===> Epoch[63](12/32): Loss: 0.0404\n",
            "===> Epoch[63](13/32): Loss: 0.0370\n",
            "===> Epoch[63](14/32): Loss: 0.0491\n",
            "===> Epoch[63](15/32): Loss: 0.0165\n",
            "===> Epoch[63](16/32): Loss: 0.0155\n",
            "===> Epoch[63](17/32): Loss: 0.0150\n",
            "===> Epoch[63](18/32): Loss: 0.0252\n",
            "===> Epoch[63](19/32): Loss: 0.0237\n",
            "===> Epoch[63](20/32): Loss: 0.0555\n",
            "===> Epoch[63](21/32): Loss: 0.0433\n",
            "===> Epoch[63](22/32): Loss: 0.0465\n",
            "===> Epoch[63](23/32): Loss: 0.0433\n",
            "===> Epoch[63](24/32): Loss: 0.0311\n",
            "===> Epoch[63](25/32): Loss: 0.0472\n",
            "===> Epoch[63](26/32): Loss: 0.0369\n",
            "===> Epoch[63](27/32): Loss: 0.0333\n",
            "===> Epoch[63](28/32): Loss: 0.0351\n",
            "===> Epoch[63](29/32): Loss: 0.0282\n",
            "===> Epoch[63](30/32): Loss: 0.0344\n",
            "===> Epoch[63](31/32): Loss: 0.0415\n",
            "===> Epoch[63](32/32): Loss: 0.0330\n",
            "===> Epoch 63 Complete: Avg. Loss: 0.0352\n",
            "===> Epoch[64](1/32): Loss: 0.0203\n",
            "===> Epoch[64](2/32): Loss: 0.0244\n",
            "===> Epoch[64](3/32): Loss: 0.0186\n",
            "===> Epoch[64](4/32): Loss: 0.0388\n",
            "===> Epoch[64](5/32): Loss: 0.0230\n",
            "===> Epoch[64](6/32): Loss: 0.0359\n",
            "===> Epoch[64](7/32): Loss: 0.0339\n",
            "===> Epoch[64](8/32): Loss: 0.0403\n",
            "===> Epoch[64](9/32): Loss: 0.0392\n",
            "===> Epoch[64](10/32): Loss: 0.0769\n",
            "===> Epoch[64](11/32): Loss: 0.0438\n",
            "===> Epoch[64](12/32): Loss: 0.0404\n",
            "===> Epoch[64](13/32): Loss: 0.0370\n",
            "===> Epoch[64](14/32): Loss: 0.0491\n",
            "===> Epoch[64](15/32): Loss: 0.0165\n",
            "===> Epoch[64](16/32): Loss: 0.0155\n",
            "===> Epoch[64](17/32): Loss: 0.0150\n",
            "===> Epoch[64](18/32): Loss: 0.0252\n",
            "===> Epoch[64](19/32): Loss: 0.0237\n",
            "===> Epoch[64](20/32): Loss: 0.0554\n",
            "===> Epoch[64](21/32): Loss: 0.0433\n",
            "===> Epoch[64](22/32): Loss: 0.0463\n",
            "===> Epoch[64](23/32): Loss: 0.0432\n",
            "===> Epoch[64](24/32): Loss: 0.0311\n",
            "===> Epoch[64](25/32): Loss: 0.0471\n",
            "===> Epoch[64](26/32): Loss: 0.0368\n",
            "===> Epoch[64](27/32): Loss: 0.0332\n",
            "===> Epoch[64](28/32): Loss: 0.0351\n",
            "===> Epoch[64](29/32): Loss: 0.0281\n",
            "===> Epoch[64](30/32): Loss: 0.0344\n",
            "===> Epoch[64](31/32): Loss: 0.0414\n",
            "===> Epoch[64](32/32): Loss: 0.0330\n",
            "===> Epoch 64 Complete: Avg. Loss: 0.0352\n",
            "===> Epoch[65](1/32): Loss: 0.0202\n",
            "===> Epoch[65](2/32): Loss: 0.0243\n",
            "===> Epoch[65](3/32): Loss: 0.0186\n",
            "===> Epoch[65](4/32): Loss: 0.0387\n",
            "===> Epoch[65](5/32): Loss: 0.0230\n",
            "===> Epoch[65](6/32): Loss: 0.0359\n",
            "===> Epoch[65](7/32): Loss: 0.0339\n",
            "===> Epoch[65](8/32): Loss: 0.0403\n",
            "===> Epoch[65](9/32): Loss: 0.0392\n",
            "===> Epoch[65](10/32): Loss: 0.0768\n",
            "===> Epoch[65](11/32): Loss: 0.0438\n",
            "===> Epoch[65](12/32): Loss: 0.0405\n",
            "===> Epoch[65](13/32): Loss: 0.0369\n",
            "===> Epoch[65](14/32): Loss: 0.0490\n",
            "===> Epoch[65](15/32): Loss: 0.0164\n",
            "===> Epoch[65](16/32): Loss: 0.0155\n",
            "===> Epoch[65](17/32): Loss: 0.0149\n",
            "===> Epoch[65](18/32): Loss: 0.0251\n",
            "===> Epoch[65](19/32): Loss: 0.0236\n",
            "===> Epoch[65](20/32): Loss: 0.0554\n",
            "===> Epoch[65](21/32): Loss: 0.0432\n",
            "===> Epoch[65](22/32): Loss: 0.0464\n",
            "===> Epoch[65](23/32): Loss: 0.0431\n",
            "===> Epoch[65](24/32): Loss: 0.0310\n",
            "===> Epoch[65](25/32): Loss: 0.0471\n",
            "===> Epoch[65](26/32): Loss: 0.0368\n",
            "===> Epoch[65](27/32): Loss: 0.0332\n",
            "===> Epoch[65](28/32): Loss: 0.0350\n",
            "===> Epoch[65](29/32): Loss: 0.0281\n",
            "===> Epoch[65](30/32): Loss: 0.0345\n",
            "===> Epoch[65](31/32): Loss: 0.0414\n",
            "===> Epoch[65](32/32): Loss: 0.0330\n",
            "===> Epoch 65 Complete: Avg. Loss: 0.0352\n",
            "===> Epoch[66](1/32): Loss: 0.0202\n",
            "===> Epoch[66](2/32): Loss: 0.0243\n",
            "===> Epoch[66](3/32): Loss: 0.0185\n",
            "===> Epoch[66](4/32): Loss: 0.0387\n",
            "===> Epoch[66](5/32): Loss: 0.0230\n",
            "===> Epoch[66](6/32): Loss: 0.0358\n",
            "===> Epoch[66](7/32): Loss: 0.0339\n",
            "===> Epoch[66](8/32): Loss: 0.0403\n",
            "===> Epoch[66](9/32): Loss: 0.0392\n",
            "===> Epoch[66](10/32): Loss: 0.0768\n",
            "===> Epoch[66](11/32): Loss: 0.0437\n",
            "===> Epoch[66](12/32): Loss: 0.0405\n",
            "===> Epoch[66](13/32): Loss: 0.0370\n",
            "===> Epoch[66](14/32): Loss: 0.0490\n",
            "===> Epoch[66](15/32): Loss: 0.0164\n",
            "===> Epoch[66](16/32): Loss: 0.0155\n",
            "===> Epoch[66](17/32): Loss: 0.0150\n",
            "===> Epoch[66](18/32): Loss: 0.0252\n",
            "===> Epoch[66](19/32): Loss: 0.0237\n",
            "===> Epoch[66](20/32): Loss: 0.0553\n",
            "===> Epoch[66](21/32): Loss: 0.0431\n",
            "===> Epoch[66](22/32): Loss: 0.0462\n",
            "===> Epoch[66](23/32): Loss: 0.0430\n",
            "===> Epoch[66](24/32): Loss: 0.0310\n",
            "===> Epoch[66](25/32): Loss: 0.0470\n",
            "===> Epoch[66](26/32): Loss: 0.0368\n",
            "===> Epoch[66](27/32): Loss: 0.0332\n",
            "===> Epoch[66](28/32): Loss: 0.0350\n",
            "===> Epoch[66](29/32): Loss: 0.0281\n",
            "===> Epoch[66](30/32): Loss: 0.0344\n",
            "===> Epoch[66](31/32): Loss: 0.0414\n",
            "===> Epoch[66](32/32): Loss: 0.0330\n",
            "===> Epoch 66 Complete: Avg. Loss: 0.0351\n",
            "===> Epoch[67](1/32): Loss: 0.0201\n",
            "===> Epoch[67](2/32): Loss: 0.0242\n",
            "===> Epoch[67](3/32): Loss: 0.0185\n",
            "===> Epoch[67](4/32): Loss: 0.0386\n",
            "===> Epoch[67](5/32): Loss: 0.0230\n",
            "===> Epoch[67](6/32): Loss: 0.0359\n",
            "===> Epoch[67](7/32): Loss: 0.0339\n",
            "===> Epoch[67](8/32): Loss: 0.0402\n",
            "===> Epoch[67](9/32): Loss: 0.0391\n",
            "===> Epoch[67](10/32): Loss: 0.0767\n",
            "===> Epoch[67](11/32): Loss: 0.0437\n",
            "===> Epoch[67](12/32): Loss: 0.0405\n",
            "===> Epoch[67](13/32): Loss: 0.0369\n",
            "===> Epoch[67](14/32): Loss: 0.0489\n",
            "===> Epoch[67](15/32): Loss: 0.0164\n",
            "===> Epoch[67](16/32): Loss: 0.0154\n",
            "===> Epoch[67](17/32): Loss: 0.0149\n",
            "===> Epoch[67](18/32): Loss: 0.0251\n",
            "===> Epoch[67](19/32): Loss: 0.0236\n",
            "===> Epoch[67](20/32): Loss: 0.0553\n",
            "===> Epoch[67](21/32): Loss: 0.0430\n",
            "===> Epoch[67](22/32): Loss: 0.0462\n",
            "===> Epoch[67](23/32): Loss: 0.0429\n",
            "===> Epoch[67](24/32): Loss: 0.0310\n",
            "===> Epoch[67](25/32): Loss: 0.0470\n",
            "===> Epoch[67](26/32): Loss: 0.0368\n",
            "===> Epoch[67](27/32): Loss: 0.0331\n",
            "===> Epoch[67](28/32): Loss: 0.0349\n",
            "===> Epoch[67](29/32): Loss: 0.0280\n",
            "===> Epoch[67](30/32): Loss: 0.0345\n",
            "===> Epoch[67](31/32): Loss: 0.0414\n",
            "===> Epoch[67](32/32): Loss: 0.0330\n",
            "===> Epoch 67 Complete: Avg. Loss: 0.0351\n",
            "===> Epoch[68](1/32): Loss: 0.0201\n",
            "===> Epoch[68](2/32): Loss: 0.0243\n",
            "===> Epoch[68](3/32): Loss: 0.0184\n",
            "===> Epoch[68](4/32): Loss: 0.0386\n",
            "===> Epoch[68](5/32): Loss: 0.0229\n",
            "===> Epoch[68](6/32): Loss: 0.0358\n",
            "===> Epoch[68](7/32): Loss: 0.0339\n",
            "===> Epoch[68](8/32): Loss: 0.0402\n",
            "===> Epoch[68](9/32): Loss: 0.0391\n",
            "===> Epoch[68](10/32): Loss: 0.0767\n",
            "===> Epoch[68](11/32): Loss: 0.0437\n",
            "===> Epoch[68](12/32): Loss: 0.0405\n",
            "===> Epoch[68](13/32): Loss: 0.0370\n",
            "===> Epoch[68](14/32): Loss: 0.0488\n",
            "===> Epoch[68](15/32): Loss: 0.0164\n",
            "===> Epoch[68](16/32): Loss: 0.0154\n",
            "===> Epoch[68](17/32): Loss: 0.0149\n",
            "===> Epoch[68](18/32): Loss: 0.0251\n",
            "===> Epoch[68](19/32): Loss: 0.0236\n",
            "===> Epoch[68](20/32): Loss: 0.0552\n",
            "===> Epoch[68](21/32): Loss: 0.0430\n",
            "===> Epoch[68](22/32): Loss: 0.0460\n",
            "===> Epoch[68](23/32): Loss: 0.0429\n",
            "===> Epoch[68](24/32): Loss: 0.0310\n",
            "===> Epoch[68](25/32): Loss: 0.0469\n",
            "===> Epoch[68](26/32): Loss: 0.0367\n",
            "===> Epoch[68](27/32): Loss: 0.0331\n",
            "===> Epoch[68](28/32): Loss: 0.0349\n",
            "===> Epoch[68](29/32): Loss: 0.0280\n",
            "===> Epoch[68](30/32): Loss: 0.0345\n",
            "===> Epoch[68](31/32): Loss: 0.0414\n",
            "===> Epoch[68](32/32): Loss: 0.0330\n",
            "===> Epoch 68 Complete: Avg. Loss: 0.0351\n",
            "===> Epoch[69](1/32): Loss: 0.0201\n",
            "===> Epoch[69](2/32): Loss: 0.0241\n",
            "===> Epoch[69](3/32): Loss: 0.0184\n",
            "===> Epoch[69](4/32): Loss: 0.0385\n",
            "===> Epoch[69](5/32): Loss: 0.0229\n",
            "===> Epoch[69](6/32): Loss: 0.0358\n",
            "===> Epoch[69](7/32): Loss: 0.0339\n",
            "===> Epoch[69](8/32): Loss: 0.0402\n",
            "===> Epoch[69](9/32): Loss: 0.0391\n",
            "===> Epoch[69](10/32): Loss: 0.0766\n",
            "===> Epoch[69](11/32): Loss: 0.0437\n",
            "===> Epoch[69](12/32): Loss: 0.0405\n",
            "===> Epoch[69](13/32): Loss: 0.0369\n",
            "===> Epoch[69](14/32): Loss: 0.0487\n",
            "===> Epoch[69](15/32): Loss: 0.0164\n",
            "===> Epoch[69](16/32): Loss: 0.0153\n",
            "===> Epoch[69](17/32): Loss: 0.0149\n",
            "===> Epoch[69](18/32): Loss: 0.0250\n",
            "===> Epoch[69](19/32): Loss: 0.0235\n",
            "===> Epoch[69](20/32): Loss: 0.0552\n",
            "===> Epoch[69](21/32): Loss: 0.0429\n",
            "===> Epoch[69](22/32): Loss: 0.0461\n",
            "===> Epoch[69](23/32): Loss: 0.0428\n",
            "===> Epoch[69](24/32): Loss: 0.0310\n",
            "===> Epoch[69](25/32): Loss: 0.0469\n",
            "===> Epoch[69](26/32): Loss: 0.0367\n",
            "===> Epoch[69](27/32): Loss: 0.0330\n",
            "===> Epoch[69](28/32): Loss: 0.0348\n",
            "===> Epoch[69](29/32): Loss: 0.0279\n",
            "===> Epoch[69](30/32): Loss: 0.0346\n",
            "===> Epoch[69](31/32): Loss: 0.0414\n",
            "===> Epoch[69](32/32): Loss: 0.0331\n",
            "===> Epoch 69 Complete: Avg. Loss: 0.0350\n",
            "===> Epoch[70](1/32): Loss: 0.0201\n",
            "===> Epoch[70](2/32): Loss: 0.0242\n",
            "===> Epoch[70](3/32): Loss: 0.0183\n",
            "===> Epoch[70](4/32): Loss: 0.0384\n",
            "===> Epoch[70](5/32): Loss: 0.0229\n",
            "===> Epoch[70](6/32): Loss: 0.0358\n",
            "===> Epoch[70](7/32): Loss: 0.0339\n",
            "===> Epoch[70](8/32): Loss: 0.0402\n",
            "===> Epoch[70](9/32): Loss: 0.0391\n",
            "===> Epoch[70](10/32): Loss: 0.0766\n",
            "===> Epoch[70](11/32): Loss: 0.0436\n",
            "===> Epoch[70](12/32): Loss: 0.0405\n",
            "===> Epoch[70](13/32): Loss: 0.0370\n",
            "===> Epoch[70](14/32): Loss: 0.0487\n",
            "===> Epoch[70](15/32): Loss: 0.0164\n",
            "===> Epoch[70](16/32): Loss: 0.0153\n",
            "===> Epoch[70](17/32): Loss: 0.0149\n",
            "===> Epoch[70](18/32): Loss: 0.0250\n",
            "===> Epoch[70](19/32): Loss: 0.0235\n",
            "===> Epoch[70](20/32): Loss: 0.0550\n",
            "===> Epoch[70](21/32): Loss: 0.0429\n",
            "===> Epoch[70](22/32): Loss: 0.0459\n",
            "===> Epoch[70](23/32): Loss: 0.0427\n",
            "===> Epoch[70](24/32): Loss: 0.0310\n",
            "===> Epoch[70](25/32): Loss: 0.0468\n",
            "===> Epoch[70](26/32): Loss: 0.0367\n",
            "===> Epoch[70](27/32): Loss: 0.0330\n",
            "===> Epoch[70](28/32): Loss: 0.0348\n",
            "===> Epoch[70](29/32): Loss: 0.0279\n",
            "===> Epoch[70](30/32): Loss: 0.0345\n",
            "===> Epoch[70](31/32): Loss: 0.0413\n",
            "===> Epoch[70](32/32): Loss: 0.0331\n",
            "===> Epoch 70 Complete: Avg. Loss: 0.0350\n",
            "===> Avg. PSNR: 14.7368 dB\n",
            "Checkpoint saved to ./model_epoch_70.pth\n",
            "===> Epoch[71](1/32): Loss: 0.0200\n",
            "===> Epoch[71](2/32): Loss: 0.0241\n",
            "===> Epoch[71](3/32): Loss: 0.0183\n",
            "===> Epoch[71](4/32): Loss: 0.0384\n",
            "===> Epoch[71](5/32): Loss: 0.0229\n",
            "===> Epoch[71](6/32): Loss: 0.0358\n",
            "===> Epoch[71](7/32): Loss: 0.0339\n",
            "===> Epoch[71](8/32): Loss: 0.0401\n",
            "===> Epoch[71](9/32): Loss: 0.0390\n",
            "===> Epoch[71](10/32): Loss: 0.0764\n",
            "===> Epoch[71](11/32): Loss: 0.0436\n",
            "===> Epoch[71](12/32): Loss: 0.0406\n",
            "===> Epoch[71](13/32): Loss: 0.0369\n",
            "===> Epoch[71](14/32): Loss: 0.0486\n",
            "===> Epoch[71](15/32): Loss: 0.0163\n",
            "===> Epoch[71](16/32): Loss: 0.0153\n",
            "===> Epoch[71](17/32): Loss: 0.0149\n",
            "===> Epoch[71](18/32): Loss: 0.0250\n",
            "===> Epoch[71](19/32): Loss: 0.0235\n",
            "===> Epoch[71](20/32): Loss: 0.0550\n",
            "===> Epoch[71](21/32): Loss: 0.0428\n",
            "===> Epoch[71](22/32): Loss: 0.0460\n",
            "===> Epoch[71](23/32): Loss: 0.0426\n",
            "===> Epoch[71](24/32): Loss: 0.0310\n",
            "===> Epoch[71](25/32): Loss: 0.0468\n",
            "===> Epoch[71](26/32): Loss: 0.0367\n",
            "===> Epoch[71](27/32): Loss: 0.0330\n",
            "===> Epoch[71](28/32): Loss: 0.0347\n",
            "===> Epoch[71](29/32): Loss: 0.0278\n",
            "===> Epoch[71](30/32): Loss: 0.0346\n",
            "===> Epoch[71](31/32): Loss: 0.0414\n",
            "===> Epoch[71](32/32): Loss: 0.0331\n",
            "===> Epoch 71 Complete: Avg. Loss: 0.0350\n",
            "===> Epoch[72](1/32): Loss: 0.0201\n",
            "===> Epoch[72](2/32): Loss: 0.0241\n",
            "===> Epoch[72](3/32): Loss: 0.0183\n",
            "===> Epoch[72](4/32): Loss: 0.0383\n",
            "===> Epoch[72](5/32): Loss: 0.0229\n",
            "===> Epoch[72](6/32): Loss: 0.0358\n",
            "===> Epoch[72](7/32): Loss: 0.0339\n",
            "===> Epoch[72](8/32): Loss: 0.0401\n",
            "===> Epoch[72](9/32): Loss: 0.0390\n",
            "===> Epoch[72](10/32): Loss: 0.0764\n",
            "===> Epoch[72](11/32): Loss: 0.0436\n",
            "===> Epoch[72](12/32): Loss: 0.0406\n",
            "===> Epoch[72](13/32): Loss: 0.0370\n",
            "===> Epoch[72](14/32): Loss: 0.0486\n",
            "===> Epoch[72](15/32): Loss: 0.0163\n",
            "===> Epoch[72](16/32): Loss: 0.0152\n",
            "===> Epoch[72](17/32): Loss: 0.0149\n",
            "===> Epoch[72](18/32): Loss: 0.0250\n",
            "===> Epoch[72](19/32): Loss: 0.0235\n",
            "===> Epoch[72](20/32): Loss: 0.0549\n",
            "===> Epoch[72](21/32): Loss: 0.0428\n",
            "===> Epoch[72](22/32): Loss: 0.0457\n",
            "===> Epoch[72](23/32): Loss: 0.0425\n",
            "===> Epoch[72](24/32): Loss: 0.0310\n",
            "===> Epoch[72](25/32): Loss: 0.0467\n",
            "===> Epoch[72](26/32): Loss: 0.0367\n",
            "===> Epoch[72](27/32): Loss: 0.0329\n",
            "===> Epoch[72](28/32): Loss: 0.0347\n",
            "===> Epoch[72](29/32): Loss: 0.0278\n",
            "===> Epoch[72](30/32): Loss: 0.0346\n",
            "===> Epoch[72](31/32): Loss: 0.0413\n",
            "===> Epoch[72](32/32): Loss: 0.0331\n",
            "===> Epoch 72 Complete: Avg. Loss: 0.0349\n",
            "===> Epoch[73](1/32): Loss: 0.0200\n",
            "===> Epoch[73](2/32): Loss: 0.0240\n",
            "===> Epoch[73](3/32): Loss: 0.0183\n",
            "===> Epoch[73](4/32): Loss: 0.0382\n",
            "===> Epoch[73](5/32): Loss: 0.0229\n",
            "===> Epoch[73](6/32): Loss: 0.0358\n",
            "===> Epoch[73](7/32): Loss: 0.0339\n",
            "===> Epoch[73](8/32): Loss: 0.0401\n",
            "===> Epoch[73](9/32): Loss: 0.0390\n",
            "===> Epoch[73](10/32): Loss: 0.0763\n",
            "===> Epoch[73](11/32): Loss: 0.0436\n",
            "===> Epoch[73](12/32): Loss: 0.0406\n",
            "===> Epoch[73](13/32): Loss: 0.0369\n",
            "===> Epoch[73](14/32): Loss: 0.0485\n",
            "===> Epoch[73](15/32): Loss: 0.0163\n",
            "===> Epoch[73](16/32): Loss: 0.0152\n",
            "===> Epoch[73](17/32): Loss: 0.0148\n",
            "===> Epoch[73](18/32): Loss: 0.0249\n",
            "===> Epoch[73](19/32): Loss: 0.0234\n",
            "===> Epoch[73](20/32): Loss: 0.0549\n",
            "===> Epoch[73](21/32): Loss: 0.0427\n",
            "===> Epoch[73](22/32): Loss: 0.0458\n",
            "===> Epoch[73](23/32): Loss: 0.0425\n",
            "===> Epoch[73](24/32): Loss: 0.0309\n",
            "===> Epoch[73](25/32): Loss: 0.0467\n",
            "===> Epoch[73](26/32): Loss: 0.0367\n",
            "===> Epoch[73](27/32): Loss: 0.0329\n",
            "===> Epoch[73](28/32): Loss: 0.0346\n",
            "===> Epoch[73](29/32): Loss: 0.0277\n",
            "===> Epoch[73](30/32): Loss: 0.0346\n",
            "===> Epoch[73](31/32): Loss: 0.0413\n",
            "===> Epoch[73](32/32): Loss: 0.0331\n",
            "===> Epoch 73 Complete: Avg. Loss: 0.0349\n",
            "===> Epoch[74](1/32): Loss: 0.0200\n",
            "===> Epoch[74](2/32): Loss: 0.0240\n",
            "===> Epoch[74](3/32): Loss: 0.0182\n",
            "===> Epoch[74](4/32): Loss: 0.0382\n",
            "===> Epoch[74](5/32): Loss: 0.0228\n",
            "===> Epoch[74](6/32): Loss: 0.0358\n",
            "===> Epoch[74](7/32): Loss: 0.0339\n",
            "===> Epoch[74](8/32): Loss: 0.0401\n",
            "===> Epoch[74](9/32): Loss: 0.0390\n",
            "===> Epoch[74](10/32): Loss: 0.0763\n",
            "===> Epoch[74](11/32): Loss: 0.0436\n",
            "===> Epoch[74](12/32): Loss: 0.0406\n",
            "===> Epoch[74](13/32): Loss: 0.0370\n",
            "===> Epoch[74](14/32): Loss: 0.0484\n",
            "===> Epoch[74](15/32): Loss: 0.0163\n",
            "===> Epoch[74](16/32): Loss: 0.0151\n",
            "===> Epoch[74](17/32): Loss: 0.0148\n",
            "===> Epoch[74](18/32): Loss: 0.0249\n",
            "===> Epoch[74](19/32): Loss: 0.0234\n",
            "===> Epoch[74](20/32): Loss: 0.0548\n",
            "===> Epoch[74](21/32): Loss: 0.0427\n",
            "===> Epoch[74](22/32): Loss: 0.0456\n",
            "===> Epoch[74](23/32): Loss: 0.0424\n",
            "===> Epoch[74](24/32): Loss: 0.0309\n",
            "===> Epoch[74](25/32): Loss: 0.0466\n",
            "===> Epoch[74](26/32): Loss: 0.0366\n",
            "===> Epoch[74](27/32): Loss: 0.0329\n",
            "===> Epoch[74](28/32): Loss: 0.0346\n",
            "===> Epoch[74](29/32): Loss: 0.0277\n",
            "===> Epoch[74](30/32): Loss: 0.0346\n",
            "===> Epoch[74](31/32): Loss: 0.0413\n",
            "===> Epoch[74](32/32): Loss: 0.0331\n",
            "===> Epoch 74 Complete: Avg. Loss: 0.0349\n",
            "===> Epoch[75](1/32): Loss: 0.0200\n",
            "===> Epoch[75](2/32): Loss: 0.0239\n",
            "===> Epoch[75](3/32): Loss: 0.0182\n",
            "===> Epoch[75](4/32): Loss: 0.0381\n",
            "===> Epoch[75](5/32): Loss: 0.0229\n",
            "===> Epoch[75](6/32): Loss: 0.0358\n",
            "===> Epoch[75](7/32): Loss: 0.0339\n",
            "===> Epoch[75](8/32): Loss: 0.0401\n",
            "===> Epoch[75](9/32): Loss: 0.0389\n",
            "===> Epoch[75](10/32): Loss: 0.0762\n",
            "===> Epoch[75](11/32): Loss: 0.0436\n",
            "===> Epoch[75](12/32): Loss: 0.0406\n",
            "===> Epoch[75](13/32): Loss: 0.0369\n",
            "===> Epoch[75](14/32): Loss: 0.0483\n",
            "===> Epoch[75](15/32): Loss: 0.0163\n",
            "===> Epoch[75](16/32): Loss: 0.0151\n",
            "===> Epoch[75](17/32): Loss: 0.0148\n",
            "===> Epoch[75](18/32): Loss: 0.0249\n",
            "===> Epoch[75](19/32): Loss: 0.0233\n",
            "===> Epoch[75](20/32): Loss: 0.0548\n",
            "===> Epoch[75](21/32): Loss: 0.0426\n",
            "===> Epoch[75](22/32): Loss: 0.0456\n",
            "===> Epoch[75](23/32): Loss: 0.0423\n",
            "===> Epoch[75](24/32): Loss: 0.0309\n",
            "===> Epoch[75](25/32): Loss: 0.0466\n",
            "===> Epoch[75](26/32): Loss: 0.0366\n",
            "===> Epoch[75](27/32): Loss: 0.0328\n",
            "===> Epoch[75](28/32): Loss: 0.0345\n",
            "===> Epoch[75](29/32): Loss: 0.0277\n",
            "===> Epoch[75](30/32): Loss: 0.0347\n",
            "===> Epoch[75](31/32): Loss: 0.0413\n",
            "===> Epoch[75](32/32): Loss: 0.0332\n",
            "===> Epoch 75 Complete: Avg. Loss: 0.0349\n",
            "===> Epoch[76](1/32): Loss: 0.0200\n",
            "===> Epoch[76](2/32): Loss: 0.0239\n",
            "===> Epoch[76](3/32): Loss: 0.0182\n",
            "===> Epoch[76](4/32): Loss: 0.0380\n",
            "===> Epoch[76](5/32): Loss: 0.0228\n",
            "===> Epoch[76](6/32): Loss: 0.0358\n",
            "===> Epoch[76](7/32): Loss: 0.0339\n",
            "===> Epoch[76](8/32): Loss: 0.0400\n",
            "===> Epoch[76](9/32): Loss: 0.0389\n",
            "===> Epoch[76](10/32): Loss: 0.0762\n",
            "===> Epoch[76](11/32): Loss: 0.0436\n",
            "===> Epoch[76](12/32): Loss: 0.0406\n",
            "===> Epoch[76](13/32): Loss: 0.0370\n",
            "===> Epoch[76](14/32): Loss: 0.0483\n",
            "===> Epoch[76](15/32): Loss: 0.0163\n",
            "===> Epoch[76](16/32): Loss: 0.0150\n",
            "===> Epoch[76](17/32): Loss: 0.0148\n",
            "===> Epoch[76](18/32): Loss: 0.0249\n",
            "===> Epoch[76](19/32): Loss: 0.0234\n",
            "===> Epoch[76](20/32): Loss: 0.0547\n",
            "===> Epoch[76](21/32): Loss: 0.0426\n",
            "===> Epoch[76](22/32): Loss: 0.0454\n",
            "===> Epoch[76](23/32): Loss: 0.0423\n",
            "===> Epoch[76](24/32): Loss: 0.0309\n",
            "===> Epoch[76](25/32): Loss: 0.0464\n",
            "===> Epoch[76](26/32): Loss: 0.0366\n",
            "===> Epoch[76](27/32): Loss: 0.0328\n",
            "===> Epoch[76](28/32): Loss: 0.0345\n",
            "===> Epoch[76](29/32): Loss: 0.0276\n",
            "===> Epoch[76](30/32): Loss: 0.0347\n",
            "===> Epoch[76](31/32): Loss: 0.0413\n",
            "===> Epoch[76](32/32): Loss: 0.0332\n",
            "===> Epoch 76 Complete: Avg. Loss: 0.0348\n",
            "===> Epoch[77](1/32): Loss: 0.0200\n",
            "===> Epoch[77](2/32): Loss: 0.0238\n",
            "===> Epoch[77](3/32): Loss: 0.0182\n",
            "===> Epoch[77](4/32): Loss: 0.0379\n",
            "===> Epoch[77](5/32): Loss: 0.0229\n",
            "===> Epoch[77](6/32): Loss: 0.0358\n",
            "===> Epoch[77](7/32): Loss: 0.0339\n",
            "===> Epoch[77](8/32): Loss: 0.0400\n",
            "===> Epoch[77](9/32): Loss: 0.0388\n",
            "===> Epoch[77](10/32): Loss: 0.0761\n",
            "===> Epoch[77](11/32): Loss: 0.0435\n",
            "===> Epoch[77](12/32): Loss: 0.0406\n",
            "===> Epoch[77](13/32): Loss: 0.0369\n",
            "===> Epoch[77](14/32): Loss: 0.0482\n",
            "===> Epoch[77](15/32): Loss: 0.0162\n",
            "===> Epoch[77](16/32): Loss: 0.0150\n",
            "===> Epoch[77](17/32): Loss: 0.0148\n",
            "===> Epoch[77](18/32): Loss: 0.0248\n",
            "===> Epoch[77](19/32): Loss: 0.0233\n",
            "===> Epoch[77](20/32): Loss: 0.0547\n",
            "===> Epoch[77](21/32): Loss: 0.0425\n",
            "===> Epoch[77](22/32): Loss: 0.0454\n",
            "===> Epoch[77](23/32): Loss: 0.0422\n",
            "===> Epoch[77](24/32): Loss: 0.0309\n",
            "===> Epoch[77](25/32): Loss: 0.0464\n",
            "===> Epoch[77](26/32): Loss: 0.0366\n",
            "===> Epoch[77](27/32): Loss: 0.0327\n",
            "===> Epoch[77](28/32): Loss: 0.0344\n",
            "===> Epoch[77](29/32): Loss: 0.0276\n",
            "===> Epoch[77](30/32): Loss: 0.0347\n",
            "===> Epoch[77](31/32): Loss: 0.0413\n",
            "===> Epoch[77](32/32): Loss: 0.0332\n",
            "===> Epoch 77 Complete: Avg. Loss: 0.0348\n",
            "===> Epoch[78](1/32): Loss: 0.0200\n",
            "===> Epoch[78](2/32): Loss: 0.0239\n",
            "===> Epoch[78](3/32): Loss: 0.0181\n",
            "===> Epoch[78](4/32): Loss: 0.0379\n",
            "===> Epoch[78](5/32): Loss: 0.0229\n",
            "===> Epoch[78](6/32): Loss: 0.0358\n",
            "===> Epoch[78](7/32): Loss: 0.0340\n",
            "===> Epoch[78](8/32): Loss: 0.0400\n",
            "===> Epoch[78](9/32): Loss: 0.0388\n",
            "===> Epoch[78](10/32): Loss: 0.0761\n",
            "===> Epoch[78](11/32): Loss: 0.0435\n",
            "===> Epoch[78](12/32): Loss: 0.0406\n",
            "===> Epoch[78](13/32): Loss: 0.0370\n",
            "===> Epoch[78](14/32): Loss: 0.0481\n",
            "===> Epoch[78](15/32): Loss: 0.0162\n",
            "===> Epoch[78](16/32): Loss: 0.0149\n",
            "===> Epoch[78](17/32): Loss: 0.0148\n",
            "===> Epoch[78](18/32): Loss: 0.0248\n",
            "===> Epoch[78](19/32): Loss: 0.0233\n",
            "===> Epoch[78](20/32): Loss: 0.0545\n",
            "===> Epoch[78](21/32): Loss: 0.0425\n",
            "===> Epoch[78](22/32): Loss: 0.0452\n",
            "===> Epoch[78](23/32): Loss: 0.0422\n",
            "===> Epoch[78](24/32): Loss: 0.0308\n",
            "===> Epoch[78](25/32): Loss: 0.0463\n",
            "===> Epoch[78](26/32): Loss: 0.0365\n",
            "===> Epoch[78](27/32): Loss: 0.0327\n",
            "===> Epoch[78](28/32): Loss: 0.0344\n",
            "===> Epoch[78](29/32): Loss: 0.0276\n",
            "===> Epoch[78](30/32): Loss: 0.0348\n",
            "===> Epoch[78](31/32): Loss: 0.0412\n",
            "===> Epoch[78](32/32): Loss: 0.0332\n",
            "===> Epoch 78 Complete: Avg. Loss: 0.0348\n",
            "===> Epoch[79](1/32): Loss: 0.0200\n",
            "===> Epoch[79](2/32): Loss: 0.0237\n",
            "===> Epoch[79](3/32): Loss: 0.0181\n",
            "===> Epoch[79](4/32): Loss: 0.0378\n",
            "===> Epoch[79](5/32): Loss: 0.0229\n",
            "===> Epoch[79](6/32): Loss: 0.0358\n",
            "===> Epoch[79](7/32): Loss: 0.0339\n",
            "===> Epoch[79](8/32): Loss: 0.0400\n",
            "===> Epoch[79](9/32): Loss: 0.0388\n",
            "===> Epoch[79](10/32): Loss: 0.0760\n",
            "===> Epoch[79](11/32): Loss: 0.0435\n",
            "===> Epoch[79](12/32): Loss: 0.0407\n",
            "===> Epoch[79](13/32): Loss: 0.0369\n",
            "===> Epoch[79](14/32): Loss: 0.0480\n",
            "===> Epoch[79](15/32): Loss: 0.0162\n",
            "===> Epoch[79](16/32): Loss: 0.0149\n",
            "===> Epoch[79](17/32): Loss: 0.0147\n",
            "===> Epoch[79](18/32): Loss: 0.0248\n",
            "===> Epoch[79](19/32): Loss: 0.0232\n",
            "===> Epoch[79](20/32): Loss: 0.0545\n",
            "===> Epoch[79](21/32): Loss: 0.0424\n",
            "===> Epoch[79](22/32): Loss: 0.0452\n",
            "===> Epoch[79](23/32): Loss: 0.0421\n",
            "===> Epoch[79](24/32): Loss: 0.0308\n",
            "===> Epoch[79](25/32): Loss: 0.0463\n",
            "===> Epoch[79](26/32): Loss: 0.0365\n",
            "===> Epoch[79](27/32): Loss: 0.0327\n",
            "===> Epoch[79](28/32): Loss: 0.0343\n",
            "===> Epoch[79](29/32): Loss: 0.0276\n",
            "===> Epoch[79](30/32): Loss: 0.0348\n",
            "===> Epoch[79](31/32): Loss: 0.0413\n",
            "===> Epoch[79](32/32): Loss: 0.0333\n",
            "===> Epoch 79 Complete: Avg. Loss: 0.0347\n",
            "===> Epoch[80](1/32): Loss: 0.0200\n",
            "===> Epoch[80](2/32): Loss: 0.0238\n",
            "===> Epoch[80](3/32): Loss: 0.0181\n",
            "===> Epoch[80](4/32): Loss: 0.0377\n",
            "===> Epoch[80](5/32): Loss: 0.0228\n",
            "===> Epoch[80](6/32): Loss: 0.0358\n",
            "===> Epoch[80](7/32): Loss: 0.0340\n",
            "===> Epoch[80](8/32): Loss: 0.0400\n",
            "===> Epoch[80](9/32): Loss: 0.0387\n",
            "===> Epoch[80](10/32): Loss: 0.0760\n",
            "===> Epoch[80](11/32): Loss: 0.0434\n",
            "===> Epoch[80](12/32): Loss: 0.0407\n",
            "===> Epoch[80](13/32): Loss: 0.0370\n",
            "===> Epoch[80](14/32): Loss: 0.0480\n",
            "===> Epoch[80](15/32): Loss: 0.0162\n",
            "===> Epoch[80](16/32): Loss: 0.0149\n",
            "===> Epoch[80](17/32): Loss: 0.0147\n",
            "===> Epoch[80](18/32): Loss: 0.0248\n",
            "===> Epoch[80](19/32): Loss: 0.0232\n",
            "===> Epoch[80](20/32): Loss: 0.0544\n",
            "===> Epoch[80](21/32): Loss: 0.0424\n",
            "===> Epoch[80](22/32): Loss: 0.0450\n",
            "===> Epoch[80](23/32): Loss: 0.0421\n",
            "===> Epoch[80](24/32): Loss: 0.0308\n",
            "===> Epoch[80](25/32): Loss: 0.0462\n",
            "===> Epoch[80](26/32): Loss: 0.0365\n",
            "===> Epoch[80](27/32): Loss: 0.0327\n",
            "===> Epoch[80](28/32): Loss: 0.0343\n",
            "===> Epoch[80](29/32): Loss: 0.0275\n",
            "===> Epoch[80](30/32): Loss: 0.0348\n",
            "===> Epoch[80](31/32): Loss: 0.0412\n",
            "===> Epoch[80](32/32): Loss: 0.0333\n",
            "===> Epoch 80 Complete: Avg. Loss: 0.0347\n",
            "===> Avg. PSNR: 14.6131 dB\n",
            "Checkpoint saved to ./model_epoch_80.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "jq_pQZq2ykg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdoWrLacaaoC",
        "outputId": "aca1cb3b-fe61-4ecc-977b-84a803f64644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Uv7p1pvHabky"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}